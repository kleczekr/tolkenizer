{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "foxbook_topic_modeling.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqqHq6fF9CvXLc5SfGFKD/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kleczekr/tolkenizer/blob/master/foxbook_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF9ODEjfyawZ",
        "colab_type": "text"
      },
      "source": [
        "### Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOeDyXNNxKVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scikit-Learn solution:\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "class SklearnTopicModels(object):\n",
        "  def __init__(self, n_topics=50):\n",
        "    '''\n",
        "    n_topics is the desired number of topics\n",
        "    '''\n",
        "    self.n_topics = n_topics\n",
        "    self.model = Pipeline([\n",
        "                           ('norm', TextNormalizer()),\n",
        "                           ('vect', CountVectorizer(tokenizer=identity,\n",
        "                                                    preprocessor=None,\n",
        "                                                    lowercase=False)),\n",
        "                           ('model', LatentDirichletAllocation(n_topics=self.n_topics))\n",
        "    ])\n",
        "\n",
        "  def fit_transform(self, documents):\n",
        "    self.model.fit_transform(documents)\n",
        "    return self.model\n",
        "\n",
        "  def get_topics(self, n=25):\n",
        "    '''\n",
        "    n is the number of top terms to show for each topic\n",
        "    '''\n",
        "    vectorizer = self.model.named_steps['vect']\n",
        "    model = self.model.steps[-1][1]\n",
        "    names = vectorizer.get_feature_names()\n",
        "    topics = dict()\n",
        "\n",
        "    for idx, topic in enumerate(model.components_):\n",
        "      features = topic.argsort()[:-(n - 1): -1]\n",
        "      tokens = [names[i] for i in features]\n",
        "      topics[idx] = tokens\n",
        "    \n",
        "    return topics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  corpus = PickledCorpusReader('corpus/')\n",
        "\n",
        "  lda = SklearnTopicModels()\n",
        "  documents = corpus.docs()\n",
        "\n",
        "  lda.fit_transform(documents)\n",
        "  topics = lda.get_topics()\n",
        "  for topic, terms in topics.items():\n",
        "    print('Topic #{}:'.format(topic+1))\n",
        "    print(terms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S1HweZD0mk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gensim implementation\n",
        "class GensimTfidfVectorizer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, dirpath='.', tofull=False):\n",
        "    '''\n",
        "    pass in a directory that holds the lexicon in corpus.dict and the\n",
        "    tf-idf model in tfidf.model.\n",
        "\n",
        "    set tofull=True if the next thing is a Scikit-Learn estimator\n",
        "    otherwise keep False if the next thing is a Gensim model.\n",
        "    '''\n",
        "    self._lexicon_path = os.path.join(dirpath, 'corpus.dict')\n",
        "    self._tfidf_path = os.path.join(dirpath, 'tfidf.model')\n",
        "\n",
        "    self.lexicon = None\n",
        "    self.tfidf = None\n",
        "    self.tofull = tofull\n",
        "\n",
        "    self.load()\n",
        "\n",
        "  def load(self):\n",
        "    if os.path.exists(self._lexicon_path):\n",
        "      self.lexicon = Dictionary.load(self._lexicon_path)\n",
        "\n",
        "    if os.path.exists(self._tfidf_path):\n",
        "      self.tfidf = TfidsModel().load(self._tfidf_path)\n",
        "\n",
        "  def save(self):\n",
        "    self.lexicon.save(self._lexicon_path)\n",
        "    self.tfidf.save(self._tfidf_path)\n",
        "\n",
        "  def fit(self, documents, labels=None):\n",
        "    self.lexicon = Dictionary(documents)\n",
        "    self.tfidf = TfidfModel([\n",
        "                             self.lexicon.doc2bow(doc)\n",
        "                             for doc in documents],\n",
        "                            id2word = self.lexicon)\n",
        "    self.save()\n",
        "    return self\n",
        "\n",
        "  def transform(self, documents):\n",
        "    def generator():\n",
        "      for document in documents:\n",
        "        vec = self.tfidf[self.lexicon.doc2bow(document)]\n",
        "        if self.tofull:\n",
        "          yield sparse2full(vec)\n",
        "        else:\n",
        "          yield vec\n",
        "    return list(generator())\n",
        "\n",
        "from gensim.sklearn_api import ldamodel\n",
        "\n",
        "class GensimTopicModels(object):\n",
        "\n",
        "  def __init__(self, n_topics = 50):\n",
        "    '''\n",
        "    n_topics is the desired number of topics\n",
        "    '''\n",
        "    self.n_topics = n_topics\n",
        "    self.model = Pipeline([\n",
        "                          ('norm', TextNormalizer()),\n",
        "                          ('vect', GensimTfidfVectorizer()),\n",
        "                          ('model', ldamodel.LdaTransformer(num_topics = self.n_topics))\n",
        "    ])\n",
        "\n",
        "  def fit(self, documents):\n",
        "    self.model.fit(documents)\n",
        "    return self.model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  corpus = PickledCorpusReader('../corpus')\n",
        "  gensim_lda = GensimTopicModels()\n",
        "  docs = [\n",
        "          list(corpus.docs(fileids = fileid))[0]\n",
        "          for fileid in corpus.fileids()\n",
        "  ]\n",
        "\n",
        "  gensim_lda.fit(docs)\n",
        "\n",
        "def get_topics(vectorized_corpus, model):\n",
        "  from operator import itemgetter\n",
        "  topics = [\n",
        "            max(model[doc], key=itemgetter(1))[0]\n",
        "            for doc in vectorized_corpus\n",
        "  ]\n",
        "  return topics\n",
        "\n",
        "lda = gensim_lda.model.named_steps['model'].gensim_model\n",
        "\n",
        "corpus = [\n",
        "          gensim_lda.model.named_steps['vect'].lexicon.doc2bow(doc)\n",
        "          for doc in gensim_lda.model.named_steps['norm'].transform(docs)\n",
        "]\n",
        "\n",
        "topics = get_topics(corpus, lda)\n",
        "\n",
        "for topic, doc in zip(topics, docs):\n",
        "  print('Topic: {}'.format(topic))\n",
        "  print(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3QyGpMd7Dlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizing the gensim solution:\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "lda = gensim_lda.model.named_steps['model'].gensim_model\n",
        "\n",
        "corpus = [\n",
        "          gensim_lda.model.named_steps['vect'].lexicon.doc2bow(doc)\n",
        "          for doc in gensim_lda.model.named_steps['norm'].transform(docs)\n",
        "]\n",
        "\n",
        "lexicon = gensim_lda.model.named_steps['vect'].lexicon\n",
        "\n",
        "data = pyLDAvis.gensim.prepare(model, corpus, lexicon)\n",
        "pyLDAvis.display(data)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi48fwSr11oY",
        "colab_type": "text"
      },
      "source": [
        "### Latent Semantic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0SZkTTq7u6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scikit-Learn option:\n",
        "class SklearnTopicModels(object):\n",
        "\n",
        "  def __init__(self, n_topics = 50, estimator='LDA'):\n",
        "    '''\n",
        "    n_topics is the desired number of topics\n",
        "    to use latent semantic analysis, set estimator to 'LSA',\n",
        "    otherwise, defaults to latent dirichlet allocation (lda)\n",
        "    '''\n",
        "    self.n_topics = n_topics\n",
        "\n",
        "    if estimator == 'LSA':\n",
        "      self.estimator = TruncatedSVD(n_components=self.n_topics)\n",
        "    else:\n",
        "      self.estimator = LatentDirichletAllocation(n_topics = self.n_topics)\n",
        "\n",
        "    self.model = Pipeline([\n",
        "                           ('norm', TextNormalizer()),\n",
        "                           ('tfidf', CountVectorizer(tokenizer=identity,\n",
        "                                                     preprocessor=None,\n",
        "                                                     lowercase=False)),\n",
        "                           ('model', self.estimator)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AygjKvHm87by",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the gensim way\n",
        "from gensim.sklearn_api import lsimodel, ldamodel\n",
        "\n",
        "class GensimTopicModels(object):\n",
        "\n",
        "  def __init__(self, n_topics=50, estimator='LDA'):\n",
        "    '''\n",
        "    n_topics is the desired number of topics\n",
        "    to use latent semantic analysis, set estimator to 'LSA'\n",
        "    otherwise defaults to latent dirichlet allocation\n",
        "    '''\n",
        "    self.n_topics = n_topics\n",
        "\n",
        "    if estimator == 'LSA':\n",
        "      self.estimator = lsimodel.LsiTransformer(num_topiics=self.n_topics)\n",
        "    else:\n",
        "      self.estimator = ldamodel.LdaTransformer(num_topics=self.n_topics)\n",
        "\n",
        "    self.model = Pipeline([\n",
        "                           ('norm', TextNoralizer()),\n",
        "                           ('vect', GensimTfidfVectorizer()),\n",
        "                           ('model', self.estimator)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_bltFSC9web",
        "colab_type": "text"
      },
      "source": [
        "### Non-Negative Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LORnGBqu9zSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "class SklearnTopicModels(object):\n",
        "\n",
        "  def __init__(self, n_topics=50, estimator='LDA'):\n",
        "    '''\n",
        "    n_topics is the desired number of topics\n",
        "    to use latent semantic analysis, set estimator to 'LSA'\n",
        "    to use non-negative matrix factorization, set estimator to 'NMF\n",
        "    otherwise, defaults to latent dirichlet allocation\n",
        "    '''\n",
        "    self.n_topics = n_topics\n",
        "    \n",
        "    if estimator == 'LSA':\n",
        "      self.estimator = TruncatedSVD(n_components=self.n_topics)\n",
        "    elif estimator == 'NMF':\n",
        "      self.estimator = NMF(n_components=self.n_topics)\n",
        "    else:\n",
        "      self.estimator = LatentDirichletAllocation(n_topics=self.n_topics)\n",
        "\n",
        "    self.model = Pipeline([\n",
        "                           ('norm', TextNormalizer()),\n",
        "                           ('tfidf', CountVectorizer(tokenizer=identity,\n",
        "                                                     preprocessor=None,\n",
        "                                                     lowercase=False)),\n",
        "                           ('model', self.estimator)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}