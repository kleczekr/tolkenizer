{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vectorization_again.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnsLPs6COOKq9lx43keVty",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kleczekr/tolkenizer/blob/master/vectorization_again.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQQg6sba-uLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bf7wQjX-ea_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def describe(self, fileids = None, categories = None):\n",
        "#   counts = nltk.FreqDist()\n",
        "#   tokens = nltk.FreqDist()\n",
        "\n",
        "#   for para in self.paras(fileids, categories):\n",
        "#     counts['paras'] += 1\n",
        "\n",
        "#     for sent in para:\n",
        "#       counts['sents'] += 1\n",
        "\n",
        "#       for word, tag in sent:\n",
        "#         counts['words'] += 1\n",
        "#         tokens[word] += 1\n",
        "\n",
        "#   return {\n",
        "#       'paras':  counts['paras'],\n",
        "#       'sents':  counts['sents'],\n",
        "#       'words':  counts['words'],\n",
        "#       'vocab':  len(tokens),\n",
        "#       'lexdiv': float(counts['words']) / float(len(tokens)),\n",
        "#       'sspar':  float(counts['sents']) / float(counts['paras'])\n",
        "#   }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdgZzKvr4Eo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "  stem = nltk.stem.SnowballStemmer('english')\n",
        "  text = text.lower()\n",
        "\n",
        "  for token in nltk.word_tokenize(text):\n",
        "    if token in string.punctuation: continue\n",
        "    yield stem.stem(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CGdnmL04EmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "          'The elephant potatoes saw sneeze sneeze sample sentence full stop boom.',\n",
        "          'Bats echolocation see see bat sneeze see!',\n",
        "          'This is another sample sentence but might be different from the sentence that is in the book.'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdwoxXBW6hvP",
        "colab_type": "text"
      },
      "source": [
        "### Frequency vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRMYwzCJ4Ei8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK solution:\n",
        "from collections import defaultdict\n",
        "\n",
        "def vectorize(doc):\n",
        "  # defaultdict(int) returns 0 for a key that has not been assigned yet\n",
        "  features = defaultdict(int)\n",
        "  for token in tokenize(doc):\n",
        "    features[token] += 1\n",
        "  return features\n",
        "\n",
        "vectors = map(vectorize, corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFVvzmPT4Ef5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scikit-Learn solution:\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcQ-TFGe9RAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gensim solution:\n",
        "import gensim\n",
        "\n",
        "corpus = [tokenize(doc) for doc in corpus]\n",
        "id2word = gensim.corpora.Dictionary(corpus)\n",
        "vectors = [\n",
        "           # gensim freq encoder is doc2bow; takes single doc instance\n",
        "           id2word.doc2bow(doc) for doc in corpus\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlrGznQ66ptd",
        "colab_type": "text"
      },
      "source": [
        "### One-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGBvYoTn6tLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK solution:\n",
        "def vectorize(doc):\n",
        "  return {\n",
        "      token: True\n",
        "      for token in doc\n",
        "  }\n",
        "\n",
        "vectors = map(vectorize, corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4PWiv3z6shX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scikit-Learn solution:\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "freq = CountVectorizer()\n",
        "corpus = freq.fit_transform(corpus)\n",
        "\n",
        "# Binarizer converts frequency values to 1\n",
        "onehot = Binarizer()\n",
        "corpus = onehot.fit_transform(corpus.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShPGHYUo-2yO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gensim solution:\n",
        "corpus = [tokenize(doc) for doc in corpus]\n",
        "id2word = gensim.corpora.Dictionary(corpus)\n",
        "vectors = [\n",
        "           [(token[0], 1) for token in id2word.doc2bow(doc)]\n",
        "           for doc in corpus\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5nlM1aN6tt-",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoTqsiGm6wwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK solution:\n",
        "from nltk.text import TextCollection\n",
        "\n",
        "def vectorize(corpus):\n",
        "  corpus = [tokenize(doc) for doc in corpus]\n",
        "  texts = TextCollection(corpus)\n",
        "\n",
        "  for doc in corpus:\n",
        "    yield {\n",
        "        term: texts.tf_idf(term, doc)\n",
        "        for term in doc\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3pTsB1W6x21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scikit-Learn solution:\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVZJzEIy_IUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gensim solution\n",
        "corpus = [tokenize(doc) for doc in corpus]\n",
        "lexicon = gensim.corpora.Dictionary(corpus)\n",
        "tfidf = gensim.models.TfidfModel(dictionary = lexicon, normalize = True)\n",
        "vectors = [tfidf[lexicon.doc2bow(doc)] for doc in corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVvpef7t6yTa",
        "colab_type": "text"
      },
      "source": [
        "### Distributed representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dYr4vmQ_I-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "\n",
        "corpus = [list(tokenize(doc)) for doc in corpus]\n",
        "corpus = [\n",
        "          TaggedDocument(words, ['d{}'.format(idx)])\n",
        "          for idx, words in enumerate(corpus)\n",
        "]\n",
        "\n",
        "model = Doc2Vec(corpus, size = 5, min_count = 0)\n",
        "print(model.docvecs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYRuRpvj68SK",
        "colab_type": "text"
      },
      "source": [
        "### Pipeline?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjyZdZuC6_UY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}