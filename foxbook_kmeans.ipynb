{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "foxbook_kmeans.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOH69B3ujxQ5Ffu0/ybrtbn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kleczekr/tolkenizer/blob/master/foxbook_kmeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGRP_DtsZXfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.cluster import KMeansClusterer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "962464pRZzqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KMeansClusters(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, k=7):\n",
        "    '''\n",
        "    k is the number of clusters\n",
        "    model is the implementation of K-means\n",
        "    '''\n",
        "    self.k = k\n",
        "    self.distance = nltk.cluster.util.cosine_distance\n",
        "    self.model = KMeansClusterer(self.k, self.distance, avoid_empty_clusters=True)\n",
        "  def fit(self, documents, labels=None):\n",
        "    return self\n",
        "  def transform(self, documents):\n",
        "    '''\n",
        "    fits the K-means model to one-hot vectorized documents\n",
        "    '''\n",
        "    return self.model.cluster(documents, assign_clusters=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsqK4AeDa-1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, language='english'):\n",
        "    self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "  def is_punct(self, token):\n",
        "    return all(unicodedata.category(char).startswith('P') for char in token)\n",
        "  def is_stopword(self, token):\n",
        "    return token.lower() in self.stopwords\n",
        "  def normalize(self, document):\n",
        "    return [\n",
        "            self.lemmatize(token, tag).lower()\n",
        "            for paragraph in document\n",
        "            for sentence in paragraph\n",
        "            for (token, tag) in sentence\n",
        "            if not self.is_punct(token) and not self.is_stopword(token)\n",
        "    ]\n",
        "  def lemmatize(self, token, pos_tag):\n",
        "    tag = {\n",
        "        'N': wn.NOUN,\n",
        "        'V': wn.VERB,\n",
        "        'R': wn.ADV,\n",
        "        'J': wn.ADJ\n",
        "    }.get(pos_tag[0], wn.NOUN)\n",
        "    return self.lemmatizer.lemmatize(token, tag)\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  def transform(self, documents):\n",
        "    return [' '.join(self.normalize(doc)) for doc in documents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cS9FDS_c7XG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OneHotVectorizer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    self.vectorizer = CountVectorizer(binary=True)\n",
        "  def fit(self, documents, labels=None):\n",
        "    return self\n",
        "  def transform(self, documents):\n",
        "    freqs = self.vectorizer.fit_transform(documents)\n",
        "    return [freq.toarray()[0] for freq in freqs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzwAlRKTdaF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = PickledCorpusReader('../corpus')\n",
        "docs = corpus.docs(categories=['news'])\n",
        "\n",
        "model = Pipeline([\n",
        "                  ('norm', TextNormalizer()),\n",
        "                  ('vect', OneHotVectorizer()),\n",
        "                  ('clusters', KMeansClusters(k=7))\n",
        "])\n",
        "\n",
        "clusters = model.fit_transform(docs)\n",
        "pickles = list(corpus.fileids(categories=['news']))\n",
        "for idx, cluster in enumerate(clusters):\n",
        "  print('Document \"{}\" assigned to cluster {}.'.format(pickles[idx,cluster]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}