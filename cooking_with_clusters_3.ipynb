{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cooking_with_clusters_3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRX1JrJZHEXD98wVGAnir9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kleczekr/tolkenizer/blob/master/cooking_with_clusters_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3mdLDwdh6KZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9198a739-1182-46d1-ccd3-4401a03151f6"
      },
      "source": [
        "import nltk.data\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "from urllib import request"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o61BrNaowQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url_alice = 'https://www.gutenberg.org/files/11/11-0.txt'\n",
        "url_moby = 'https://www.gutenberg.org/files/2701/2701-0.txt'\n",
        "# opening the txt files\n",
        "response_alice = request.urlopen(url_alice)\n",
        "response_moby = request.urlopen(url_moby)\n",
        "# reading the files into raw variables as strings\n",
        "raw_alice = response_alice.read().decode('utf8')\n",
        "raw_moby = response_moby.read().decode('utf8')\n",
        "# Split the raw files into lists of sentences\n",
        "tokenized_alice = sent_tokenize(raw_alice)\n",
        "tokenized_moby = sent_tokenize(raw_moby)\n",
        "# remove the contents\n",
        "tokenized_alice = tokenized_alice[14:]\n",
        "tokenized_moby = tokenized_moby[275:]\n",
        "# join the lists\n",
        "tokenized_joint = tokenized_alice + tokenized_moby\n",
        "# split joint list into lists of words\n",
        "word_split_joint = [sentence.split() for sentence in tokenized_joint]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAWO7Diyo7Cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We1B8YRqrNi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Word2Vec(word_split_joint, min_count=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux-_hjIwrTij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d21f908f-446b-43a6-9b39-bd4a75fdd215"
      },
      "source": [
        "# summarize the loaded model\n",
        "print(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=36062, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMi80LpqrXVh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "outputId": "3deb8360-8a3c-45f1-e110-010a09ead017"
      },
      "source": [
        "# summarize vocabulary\n",
        "words = list(model.wv.vocab)\n",
        "# print random sample\n",
        "from random import sample\n",
        "\n",
        "for word in sample(words, 50):\n",
        "  print(word)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "peace\n",
            "PACIFIC,\n",
            "traveller.\n",
            "Arthur\n",
            "flitting\n",
            "demonstrations\n",
            "divine\n",
            "Its\n",
            "wasn’t;\n",
            "ME,”\n",
            "event—in\n",
            "reiterated\n",
            "moments.\n",
            "_Requin_.\n",
            "sufferable\n",
            "Mab.\n",
            "probable,\n",
            "obtains\n",
            "strange,\n",
            "“_Something_”\n",
            "Perish?\n",
            "groping\n",
            "‘Let\n",
            "intermixed\n",
            "rehearsing—singing,\n",
            "sciences,\n",
            "shares\n",
            "coin\n",
            "flung\n",
            "Spanishly\n",
            "heads—namely,\n",
            "prize.\n",
            "Wonder\n",
            "describes\n",
            "to,\n",
            "astir\n",
            "Spermacetti\n",
            "relief,\n",
            "sail-needles\n",
            "in\n",
            "miasmas,\n",
            "Or\n",
            "escape—blow\n",
            "voices,\n",
            "worships\n",
            "computer\n",
            "moss-bearded\n",
            "going.\n",
            "such,\n",
            "ungrateful;\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1P0_RHWsSRW",
        "colab_type": "text"
      },
      "source": [
        "This is very unsatisfactory. We see not only stopwords, we see a lot of uppercase letters, punctuation, apparent encoding errors which ended up causing problems in our list of words. We need to clean this mess.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42BzA2UFrdSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import string\n",
        "# table = str.maketrans('', '', string.punctuation)\n",
        "# words = [w.translate(table) for w in words]\n",
        "# for word in sample(words, 50):\n",
        "#   print(word)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_T6diZAF-iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# words = [word.lower() for word in words]\n",
        "# for word in sample(words, 50):\n",
        "#   print(word)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIs0CX-fGHit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# another approach, with NLTK:\n",
        "# remove all tokens that are not alphabetic\n",
        "# words = [word for word in words if word.isalpha()]\n",
        "# for word in sample(words, 50):\n",
        "#   print(word)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-w-SZx9HnN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9719f1ba-3950-4aa4-fc8f-e5e086a23367"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-zi9GjiGhIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "outputId": "2b223107-3a94-4691-e8f8-b3374c34fbf7"
      },
      "source": [
        "# full cleanup pipeline:\n",
        "# convert to lower case\n",
        "words = [word.lower() for word in words]\n",
        "# remove punctuation from each word\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "words = [word.translate(table) for word in words]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "words = [word for word in words if word.isalpha()]\n",
        "# filter out stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in words if not word in stop_words]\n",
        "for word in sample(words, 50):\n",
        "  print(word)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gliding\n",
            "cupbearers\n",
            "danger\n",
            "cathedrals\n",
            "lock\n",
            "palest\n",
            "undulations\n",
            "requiring\n",
            "appear\n",
            "alive\n",
            "thoughtfulness\n",
            "crumpled\n",
            "ebook\n",
            "waning\n",
            "quivering\n",
            "shoreless\n",
            "savagery\n",
            "upward\n",
            "seaport\n",
            "sandhills\n",
            "harvest\n",
            "punctured\n",
            "indirect\n",
            "quietly\n",
            "stars\n",
            "glue\n",
            "gladly\n",
            "askance\n",
            "corroded\n",
            "intervals\n",
            "twinkling\n",
            "exhaust\n",
            "amputation\n",
            "interregnum\n",
            "unwonted\n",
            "matters\n",
            "ribs\n",
            "perilous\n",
            "emboldened\n",
            "stunning\n",
            "workman\n",
            "enviable\n",
            "war\n",
            "wisest\n",
            "shored\n",
            "entirely\n",
            "ere\n",
            "balloon\n",
            "obviously\n",
            "stockings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mb8AhnsHcov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "outputId": "0ef95bdb-6e03-476a-a6f8-6696bc562f37"
      },
      "source": [
        "# stemming of words\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "words = [porter.stem(word) for word in words]\n",
        "for word in sample(words, 50):\n",
        "  print(word)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "even\n",
            "notion\n",
            "spark\n",
            "moment\n",
            "abruptli\n",
            "execut\n",
            "insular\n",
            "grandiloqu\n",
            "vigil\n",
            "valparaiso\n",
            "engraft\n",
            "heaveninsult\n",
            "therebi\n",
            "profess\n",
            "tripoint\n",
            "chief\n",
            "shambl\n",
            "vessel\n",
            "vers\n",
            "squall\n",
            "assur\n",
            "squall\n",
            "cleveland\n",
            "contain\n",
            "job\n",
            "iron\n",
            "appropri\n",
            "rejoin\n",
            "cring\n",
            "therebi\n",
            "halfhiss\n",
            "slumber\n",
            "gape\n",
            "expos\n",
            "dim\n",
            "unreason\n",
            "legisl\n",
            "care\n",
            "pardon\n",
            "ugli\n",
            "element\n",
            "hearken\n",
            "high\n",
            "process\n",
            "medicin\n",
            "cat\n",
            "blanch\n",
            "paw\n",
            "gibraltar\n",
            "globe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AE0z7etI9Cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}